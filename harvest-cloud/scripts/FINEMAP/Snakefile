import pandas as pd
import numpy as np
import csv
import os


CHR_nms= ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X']

def selectUnrelated(input_kin, df, x):
        kin= pd.read_csv(input_kin, header= 0, sep= '\t', usecols= ['ID1', 'ID2', 'Kinship'])
        kin= kin.loc[kin.Kinship > 0.0884, :]
        kin= kin.loc[kin.ID1.isin(x.values)]
        kin= kin.loc[kin.ID2.isin(x.values)]
        kin= kin.loc[:, ['ID1','ID2','Kinship']]
        kin_temp= kin.copy()
        kin_temp.columns= ['ID2', 'ID1', 'Kinship']
        kin_temp= kin_temp.append(kin)
        kin_temp['n']= kin_temp.groupby('ID1')['ID1'].transform('count')
        kin_temp['nn']= kin_temp.groupby('ID2')['ID2'].transform('count')
        kin_temp.sort_values(by=['n', 'nn'], inplace= True)
        to_keep= list()
        for i in range(0, len(kin_temp.index)):
                if kin_temp.iloc[i, 0] in kin_temp.iloc[0:i, 1].values:
                        kin_temp.iloc[i, 1]= "X"
                else:
                        to_keep.append(kin_temp.iloc[i, 0])
        to_remove= [i for i in kin_temp.ID1 if i not in to_keep]
        to_remove= list(set(to_remove))
        remove= pd.DataFrame({'FID': to_remove})
        remove['IID']= remove.FID
        return remove

def maximal_independent_set(kin, kin_filter, samples):
	''
	import networkx as nx
	import networkx.algorithms.approximation as nxaa
	kin= pd.read_csv(input_kin, header= 0, sep= '\t', usecols= ['IID1', 'IID2', 'KINSHIP'])
	df= df.loc[df.KINSHIP> kin_filter, :]
	df= df.loc[((df.IID1.isin(samples)) & (df.IID2.isin(samples)), :]
	df= df[['IID1', 'IID2']]
	G= 
	

rule check_sex:
	'Het checks of X chromosome: sex imputation'
	input:
		expand('/mnt/archive/MOBAGENETICS/genotypes-base/imputed/subset/grm-high-quality-pruned/grm-high-quality-pruned.{ext}', ext= ['fam', 'bed', 'bim'])
	output:
		'/mnt/work2/pol/metaGWAS/processed_data/sex/MOBA.sexcheck'
	params:
		'/mnt/archive/MOBAGENETICS/genotypes-base/imputed/subset/grm-high-quality-pruned/grm-high-quality-pruned',
		'/mnt/work2/pol/metaGWAS/processed_data/sex/MOBA'
	shell:
		'/home/pol.sole.navais/soft/plink --bfile {params[0]} --check-sex --out {params[1]}'

rule sample_file:
	'Extract sample ids for unrelated subjects.'
	input:
		'/mnt/archive/MOBAGENETICS/genotypes-base/aux/flaglist-merged/mobagen-flaglist-n99259.txt',
		'/mnt/work2/pol/metaGWAS/processed_data/sex/MOBA.sexcheck',
		'/mnt/archive/MOBAGENETICS/genotypes-base/aux/pedigree/mobagen-ethnic-core-samples.kin0',
		'/mnt/archive/MOBAGENETICS/genotypes-base/aux/pca/mobagen-total/mobagen-total-proj-pc'
	output:
		'/mnt/work2/pol/metaGWAS/processed_data/ids/autosomes_IDS.txt',
		'/mnt/work2/pol/metaGWAS/processed_data/ids/chrX_IDS.txt'
	run:
		d= pd.read_csv(input[0], header=0, sep= '\t')
		x= pd.read_csv(input[1], header= 0, delim_whitespace= True)
		x= x.loc[x.SNPSEX== 2, :]
		d= d.loc[d.ROLE== 'FOUNDER', :]
		d= d.loc[d.genotypesOK== True, :]
		d= d.loc[d.phenoOK== True, :]
		pc= pd.read_csv(input[3], header= 0, delim_whitespace= True)
		pc.iloc[:, 2:]= abs((pc.iloc[:, 2:]-pc.iloc[:, 2:].mean())/ pc.iloc[:, 2:].std())
		pc= pc.loc[(pc.PC1<6) & (pc.PC2< 6) & (pc.PC3<6) & (pc.PC4< 6) & (pc.PC5<6) & (pc.PC6< 6)]
		d= d.loc[d.IID.isin(pc.IID), :]
		remove= selectUnrelated(input[2], d, d.IID)
		df= d.loc[~d.IID.isin(remove), :]
		df.to_csv(output[0], header= False, index= False, sep= '\t', columns= ['IID'])
		d= d.loc[d.IID.isin(x.IID), :]
		remove= selectUnrelated(input[2], d, d.IID)
		d= d.loc[~d.IID.isin(remove), :]
		d.to_csv(output[1], header= False, index= False, sep= '\t', columns= ['IID'])

rule independent_GWAS_regions:
	'Obtain a file with independent regions for top loci with a radius of 1.5 Mb.'
	input:
		'/mnt/work2/pol/metaGWAS/results/meta/Maternal_GWAMA_{pheno}.txt.gz'
	output:
		'/mnt/work2/pol/metaGWAS/results/top_regions/top_regions_FINEMAP_{pheno}.txt'
	run:
		d= pd.read_csv(input[0], sep= '\t', compression= 'gzip', usecols= ['CHR', 'POS', 'pvalue', 'nearestGene'])
		df= d.loc[d.pvalue< 5*10**-8, :]
		df.sort_values(by= 'pvalue', ascending= True, inplace= True)
		df.drop_duplicates(subset= ['CHR', 'POS'], keep= 'first', inplace= True)
		df_list= list()
		for chrom in set(df.CHR):
			d_temp= df.loc[df.CHR== chrom, :]
			positions= d_temp.POS.values
			for pos in positions:
				if pos in d_temp.POS.values:
					df_list.append(d_temp.loc[d_temp.POS== pos, :])
					d_temp= d_temp.loc[(d_temp.POS < pos - (1.5*10**6)) | (d_temp.POS> pos + (1.5 * 10**6)), :]
				else:
					continue
		x= pd.concat(df_list)
		x['pos1']= x.POS - 1.5*10**6
		x['pos2']= x.POS + 1.5*10**6
		x['CHR']= x.CHR.astype(str)
		x['CHR']= np.where(x.CHR== '23', 'X', x.CHR)
		x.to_csv(output[0], sep='\t', header= True, index= False, columns= ['CHR', 'pos1', 'pos2', 'nearestGene'])

checkpoint list_variants_sumstats:
	'List of genetic variants (chr:pos) in sumstats file.'
	input:
		'/mnt/work2/pol/metaGWAS/results/meta/Maternal_GWAMA_{pheno}.txt.gz',
		'/mnt/work2/pol/metaGWAS/results/top_regions/top_regions_FINEMAP_{pheno}.txt'
	output:
		directory('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/regions')
	params:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/regions/'
	run:
		if not os.path.exists(params[0]):
                        os.makedirs(params[0])
		d= pd.read_csv(input[0], sep= '\t', compression= 'gzip', usecols= ['CHR', 'POS', 'pvalue'])
		d['CHR']= d.CHR.astype(str)
		d['CHR']= np.where(d.CHR== '23', 'X', d.CHR)
		x= pd.read_csv(input[1], sep='\t', header= 0)
		x['CHR']= x.CHR.astype(str)
		d= pd.merge(d, x, on= 'CHR', how= 'inner')
		d= d.loc[((d.POS>= d.pos1) & (d.POS<= d.pos2)), :]
		d.sort_values(by= 'pvalue', ascending= True, inplace= True)
		d.drop_duplicates(subset= ['CHR', 'POS'], keep= 'first', inplace= True)
		d['ID']= d.CHR.map(str) + ':' + d.POS.map(str)
		for gene in set(d.nearestGene):
			temp_df= d.loc[d.nearestGene== gene, :]
			outfile= params[0] + 'chr' + ''.join([str(i) for i in (set(temp_df.CHR))]) + '_' + gene + '.txt'
#			d.to_csv(outfile, sep= '\t', header= False, index= False, columns= ['ID'])
			with open(outfile, 'w') as f:
				writer= csv.writer(f, delimiter= ' ')
				writer.writerow(temp_df['ID'])

rule extract_bgen_QCTOOL_temp:
	'Extract genotypes into BGEN file format for each top locus (3Mb around top SNP).'
	input:
		'/mnt/work2/pol/metaGWAS/processed_data/ids/autosomes_IDS.txt',
		'/mnt/work2/pol/metaGWAS/processed_data/ids/chrX_IDS.txt',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/regions/{locus_ID}.txt',
		expand('/mnt/archive/MOBAGENETICS/genotypes-base/imputed/all/bgen/{CHR}.bgen', CHR= CHR_nms)
	output:
		temp('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/geno/{locus_ID}.bgen'),
		temp('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/geno/{locus_ID}.bgen.bgi')
	run:
		d= pd.read_csv(input[2], sep= '\t', header= None, names= ['ID'])
		with open(input[2], 'r') as f:
			chrom= f.readlines()[0].split()[0].split(':')[0]
		bgen= ''.join([infile for infile in input if '/' + chrom + '.bgen' in infile])
		if chrom== 'X': samples= input[1]
		if chrom!= 'X': samples= input[0]
		shell('/home/pol.sole.navais/soft/qctool_v2.0.8/qctool -g {bgen} -incl-samples {samples} -incl-positions {input[2]} -og {output[0]}')
		shell('/home/pol.sole.navais/soft/bgen/build/apps/bgenix -g {output[0]} -index')

rule calculate_sumstats_bgen:
	''
	input:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/geno/{locus_ID}.bgen'
	output:
		temp('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/sumstats/{locus_ID}.sumstats')
	shell:
		'/home/pol.sole.navais/soft/qctool_v2.0.8/qctool -g {input[0]} -snp-stats -osnp {output[0]}'


rule format_sumstats:
	''
	input:
		'/mnt/work2/pol/metaGWAS/results/meta/Maternal_GWAMA_{pheno}.txt.gz',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/sumstats/{locus_ID}.sumstats'
	output:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/rsid/{locus_ID}_keep_rsid.txt'
	run:
		d= pd.read_csv(input[0], sep= '\t', header=0 , compression= 'gzip', usecols= ['CHR', 'POS', 'EFF', 'REF', 'pvalue'])
		d['CHR']= d['CHR'].apply(str)
		d['CHR']= np.where(d.CHR== '23', 'X', d.CHR)
		x= pd.read_csv(input[1], sep= '\t', header=0, comment= '#', usecols= ['rsid', 'chromosome', 'position', 'alleleA', 'alleleB'])
		x['chromosome']= x['chromosome'].apply(str)
		x= pd.merge(d, x, left_on= ['CHR', 'POS'], right_on= ['chromosome', 'position'], how= 'right')
                x= x.loc[(((x.EFF== x.alleleA) & (x.REF== x.alleleB)) | ((x.EFF== x.alleleB) & (x.REF== x.alleleA))), :]
		x.sort_values(by= 'pvalue', ascending= True, inplace= True)
		x.drop_duplicates(['chromosome', 'position', 'alleleA', 'alleleB'], keep= 'first', inplace= True)
                x.dropna(axis= 0, subset= ['pvalue'], inplace= True)
		with open(output[0], 'w') as f:
                        writer= csv.writer(f, delimiter= ' ')
                        writer.writerow(x['rsid'])

rule extract_bgen_BGENIX:
        'Extract genotypes into BGEN file format for each top locus (3Mb around top SNP).'
        input:
                '/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/rsid/{locus_ID}_keep_rsid.txt',
                '/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/geno/{locus_ID}.bgen',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/geno/{locus_ID}.bgen.bgi'
        output:
                temp('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/data/{locus_ID}.bgen'),
		temp('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/data/{locus_ID}.bgen.bgi')
        shell:
                '''
		/home/pol.sole.navais/soft/bgen/build/apps/bgenix -g {input[1]} -incl-rsids {input[0]} > {output[0]}
		/home/pol.sole.navais/soft/bgen/build/apps/bgenix -g {output[0]} -index
		'''

rule list_variants_bgen:
	'List variants in the same order as index.'
	input:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/data/{locus_ID}.bgen',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/data/{locus_ID}.bgen.bgi'
	output:
		temp('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/filter_var/variant_list_{locus_ID}.txt')
	shell:
		'/home/pol.sole.navais/soft/bgen/build/apps/bgenix -g {input[0]} -list | grep -v "#" > {output}'

rule z_file_LDSTORE:
	''
	input:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/filter_var/variant_list_{locus_ID}.txt'
	output:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/variant_list_{locus_ID}.z'
	run:
		d= pd.read_csv(input[0], sep= '\t', header=0, usecols= ['rsid', 'chromosome', 'position', 'first_allele', 'alternative_alleles'])
		d.columns= ['rsid', 'chromosome', 'position', 'allele1', 'allele2']
		d.to_csv(output[0], sep= ' ', header= True, index= False)


rule master_file_LDSTORE:
	'Create a master file for LDSTORE.'
	input:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/variant_list_{locus_ID}.z',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/data/{locus_ID}.bgen',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/data/{locus_ID}.bgen.bgi',
		'/mnt/work2/pol/metaGWAS/processed_data/ids/autosomes_IDS.txt',
                '/mnt/work2/pol/metaGWAS/processed_data/ids/chrX_IDS.txt'
	output:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/master/{locus_ID}.master'
	params:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/LDSTOREout/{locus_ID}.bcor',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/LDSTOREout/ld/{locus_ID}.ld',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/LDSTOREout/{locus_ID}.bdose'
	run:
		if 'chrX' in input[0]: 
			n_sample= len([line.strip() for line in open(input[4])])
			sample= input[4]
		if 'chrX' not in input[0]: 
			n_sample= len([line.strip() for line in open(input[3])])
			sample= input[3]
		shell("echo 'z;bgen;bgi;bcor;ld;n_samples;bdose' > {output[0]}")
		shell("echo '{input[0]};{input[1]};{input[2]};{params[0]};{params[1]};{n_sample};{params[2]}' >> {output[0]}")

rule bcor_LDSTORE:
	'Compute correlation matrix using LDSTORE.'
	input:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/master/{locus_ID}.master',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/variant_list_{locus_ID}.z',
                '/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/data/{locus_ID}.bgen',
                '/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/data/{locus_ID}.bgen.bgi',
                '/mnt/work2/pol/metaGWAS/processed_data/ids/autosomes_IDS.txt',
                '/mnt/work2/pol/metaGWAS/processed_data/ids/chrX_IDS.txt'
	output:
		temp('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/LDSTOREout/{locus_ID}.bcor'),
		temp('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/LDSTOREout/{locus_ID}.bdose')
	threads: 4
	shell:
		'/home/pol.sole.navais/soft/ldstore_v2.0_x86_64/ldstore_v2.0_x86_64 --in-files {input[0]} --write-bcor --write-bdose --bdose-version 1.0 --n-threads {threads} --memory 10'


rule bcor_to_LD_LDSTORE:
	'Write LD file from bcor file using LDSTORE.'
	input:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/master/{locus_ID}.master',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/LDSTOREout/{locus_ID}.bcor'
	output:
		temp('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/LDSTOREout/ld/{locus_ID}.ld')
	shell:
		'/home/pol.sole.navais/soft/ldstore_v2.0_x86_64/ldstore_v2.0_x86_64 --in-files {input[0]} --bcor-to-text'

rule z_file_FINEMAP:
        'Create z file for input in FINEMAP (GWAS sum stats).'
        input:
                '/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/variant_list_{locus_ID}.z',
                '/mnt/work2/pol/metaGWAS/results/meta/Maternal_GWAMA_{pheno}.txt.gz'
        output:
                '/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/zfile/sumstats_{locus_ID}.z'
        run:
                d= pd.read_csv(input[0], header= 0, sep= ' ')
		d['chromosome']= d.chromosome.map(str)
                x= pd.read_csv(input[1], header= 0, sep= '\t', compression= 'gzip', usecols= ['CHR', 'POS', 'EFF', 'REF', 'EAF', 'BETA', 'SE'])
		x['CHR']= x.CHR.astype(int).astype(str)
		x['CHR']= np.where(x.CHR== '23', 'X', x.CHR)
                x= pd.merge(d, x, left_on= ['chromosome', 'position'], right_on= ['CHR', 'POS'], how= 'left')
                x= x.loc[(((x.EFF== x.allele1) & (x.REF== x.allele2)) | ((x.EFF== x.allele2) & (x.REF== x.allele1))), :]
                x['maf']= np.where(x.EAF> 0.5, 1 - x.EAF, x.EAF)
                x= x[['rsid', 'chromosome', 'position', 'EFF', 'REF', 'maf', 'BETA', 'SE']]
                x.columns= ['rsid', 'chromosome', 'position', 'allele1', 'allele2', 'maf', 'beta', 'se']
                x.drop_duplicates(['chromosome', 'position', 'allele1', 'allele2'], keep= 'first', inplace= True)
                x.dropna(axis= 0, subset= ['beta'], inplace= True)
                x.to_csv(output[0], sep= ' ', header= True, index= False)

rule check_order:
	'Check order of the two z files.'
	input:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/variant_list_{locus_ID}.z',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/zfile/sumstats_{locus_ID}.z'
	output:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/order/check_order_{locus_ID}.txt'
	run:
		d= pd.read_csv(input[0], header= 0, sep= ' ')
		df= pd.read_csv(input[1], header= 0, sep= ' ')
		if all(d.rsid== df.rsid):
			open(output[0], 'a').close()
		else:
			raise ValueError('Check order of z-files')

rule master_file_FINEMAP:
	'Master file for FINEMAP.'
	input:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/zfile/sumstats_{locus_ID}.z',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/LDSTOREout/ld/{locus_ID}.ld',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/order/check_order_{locus_ID}.txt'
	output:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/master/{locus_ID}.master'
	params:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.snp',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.config',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.cred',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.log'
	run:
		if wildcards.pheno== 'allPTD': sample= 172629
                if wildcards.pheno== 'GAraw': sample= 195231
                if wildcards.pheno== 'GAnrm': sample= 149923
                if wildcards.pheno== 'postTerm': sample= 129614
                shell("echo 'z;ld;snp;config;cred;n_samples;log' > {output[0]}")
                shell("echo '{input[0]};{input[1]};{params[0]};{params[1]};{params[2]};{sample};{params[3]}' >> {output[0]}")


rule FINEMAP:
	'Finemapping using FINEMAP'
	input:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/master/{locus_ID}.master',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/LDSTOREout/ld/{locus_ID}.ld'
	output:
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.snp',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.config',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.cred5',
		'/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.log_sss'
	shell:
                '/home/pol.sole.navais/soft/finemap_v1.4_x86_64/finemap_v1.4_x86_64 --in-files {input[0]} --n-causal-snps 5 --sss --log'

def aggregate_snp(wildcards):
	'Aggregate the files from locus_ID wildcard.'
	checkpoint_output = checkpoints.list_variants_sumstats.get(**wildcards).output[0]
	return expand('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.snp', pheno= wildcards.pheno, locus_ID= glob_wildcards(os.path.join(checkpoint_output, '{locus_ID}.txt')).locus_ID)

def aggregate_conf(wildcards):
        'Aggregate the files from locus_ID wildcard.'
        checkpoint_output = checkpoints.list_variants_sumstats.get(**wildcards).output[0]
        return expand('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.conf', pheno= wildcards.pheno, locus_ID= glob_wildcards(os.path.join(checkpoint_output, '{locus_ID}.txt')).locus_ID)

def aggregate_cred(wildcards):
        'Aggregate the files from locus_ID wildcard.'
        checkpoint_output = checkpoints.list_variants_sumstats.get(**wildcards).output[0]
        return expand('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.cred5', pheno= wildcards.pheno, locus_ID= glob_wildcards(os.path.join(checkpoint_output, '{locus_ID}.txt')).locus_ID)

def aggregate_log(wildcards):
        'Aggregate the files from locus_ID wildcard.'
        checkpoint_output = checkpoints.list_variants_sumstats.get(**wildcards).output[0]
        return expand('/mnt/work2/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/FINEMAPout/results_{locus_ID}.log_sss', pheno= wildcards.pheno, locus_ID= glob_wildcards(os.path.join(checkpoint_output, '{locus_ID}.txt')).locus_ID)

rule merge_snp_outputs:
	''
	input:
		aggregate_snp
	output:
		'/mnt/work2/pol/metaGWAS/FINEMAP/results/{pheno}.snp'
	run:
		df_list= list()
		for i in input:
			d= pd.read_csv(i, sep= ' ', header=0)
			d['locus']= i.split('results_')[1].replace('.snp', '')
			df_list.append(d)
		d= pd.concat(df_list)
		d.to_csv(output[0], sep= '\t', header= True, index= False)

rule merge_conf_outputs:
        ''
        input:
                aggregate_conf
        output:
                '/mnt/work2/pol/metaGWAS/FINEMAP/results/{pheno}.conf'
        run:
                df_list= list()
                for i in input:
                        d= pd.read_csv(i, sep= ' ', header=0)
                        d['locus']= i.split('results_')[1].replace('.conf', '')
                        df_list.append(d)
                d= pd.concat(df_list)
                d.to_csv(output[0], sep= '\t', header= True, index= False)

rule merge_cred_outputs:
	''
	input:
		aggregate_cred
	output:
		'/mnt/work2/pol/metaGWAS/FINEMAP/results/{pheno}.cred5'
	shell:
		'tail -n +1 {input} > {output}'

rule merge_log_outputs:
        ''
        input:
                aggregate_log
        output:
                '/mnt/work2/pol/metaGWAS/FINEMAP/results{pheno}.log_sss'
        shell:
                'tail -n +1 {input} > {output}'







