cohort_nms= ['harvestm12', 'harvestm24', 'rotterdam1', 'rotterdam2', 'normentfeb', 'normentmay']

rule extract_HLA_region:
	'Extract HLA region from genotype files for imputation.'
	input:
		'/mnt/work2/pol/metaGWAS/pheno/maternal_{cohort}_ids.txt',
		expand('/mnt/archive/HARVEST/delivery-fhi/data/genotyped/m12/m12-genotyped.{ext}', ext= ['bed','bim','fam']),
		expand('/mnt/archive/HARVEST/delivery-fhi/data/genotyped/m24/m24-genotyped.{ext}', ext= ['bed','bim','fam']),
		expand('/mnt/archive/ROTTERDAM1/delivery-fhi/data/genotyped/genotyped.{ext}', ext= ['bed','bim','fam']),
		expand('/mnt/archive/ROTTERDAM2/delivery-fhi/data/genotyped/genotyped.{ext}', ext= ['bed','bim','fam']),
		expand('/mnt/archive/NORMENT1/delivery-fhi/data/genotyped/feb18/genotyped.{ext}', ext= ['bed','bim','fam']),
		expand('/mnt/archive/NORMENT1/delivery-fhi/data/genotyped/may16/genotyped.{ext}', ext= ['bed','bim','fam'])
	output:
		temp(expand('/mnt/work2/pol/metaGWAS/HLA/data/genotypes/{{cohort}}_HLA.{ext}', ext= ['bed', 'bim', 'fam']))
	params:
		'/mnt/archive/HARVEST/delivery-fhi/data/genotyped/m12/m12-genotyped',
		'/mnt/archive/HARVEST/delivery-fhi/data/genotyped/m24/m24-genotyped',
		'/mnt/archive/ROTTERDAM1/delivery-fhi/data/genotyped/genotyped',
		'/mnt/archive/ROTTERDAM2/delivery-fhi/data/genotyped/genotyped',
		'/mnt/archive/NORMENT1/delivery-fhi/data/genotyped/feb18/genotyped',
		'/mnt/archive/NORMENT1/delivery-fhi/data/genotyped/may16/genotyped',
		'/mnt/work2/pol/metaGWAS/HLA/data/genotypes/{cohort}_HLA'
	run:
                if 'harvestm12' == wildcards.cohort: parameter= {params[0]}
                if 'harvestm24' == wildcards.cohort: parameter= {params[1]}
                if 'rotterdam1' == wildcards.cohort: parameter= {params[2]}
                if 'rotterdam2' == wildcards.cohort: parameter= {params[3]}
                if 'normentfeb' == wildcards.cohort: parameter= {params[4]}
                if 'normentmay' == wildcards.cohort: parameter= {params[5]}
                shell('/home/pol.sole.navais/soft/plink2 --bfile {parameter} --keep {input[0]} --chr 6 --from-bp 25000000 --to-bp 35000000 --make-bed --out {params[6]}')

rule HLA_TAPAS:
        'HLA region imputation using the moduel SNP2HLA from HLA-TAPAS with 1KG as reference.'
        input:
                expand('/mnt/work2/pol/metaGWAS/HLA/data/genotypes/{{cohort}}_HLA.{ext}', ext= ['bed', 'bim', 'fam'])
        output:
                '/mnt/work2/pol/metaGWAS/HLA/data/{cohort}.bgl.phased.vcf.gz'
        params:
                '/home/pol.sole.navais/soft/HLA-TAPAS/',
                '/mnt/work2/pol/metaGWAS/HLA/data/genotypes/{cohort}_HLA',
                'resources/1000G.bglv4',
                '/mnt/work2/pol/metaGWAS/HLA/data/{cohort}'
        threads: 10
        shell:
                '''
                cd {params[0]}
                python -m SNP2HLA \
                --target {params[1]} \
                --reference {params[2]} \
                --out {params[3]} \
                --nthreads {threads} \
                --mem 10g 
                '''

rule index_vcfs:
	'Index HLA vcf files.'
	input:
		'/mnt/work2/pol/metaGWAS/HLA/data/{cohort}.bgl.phased.vcf.gz'
	output:
		'/mnt/work2/pol/metaGWAS/HLA/data/{cohort}.bgl.phased.vcf.gz.tbi'
	shell:
		'tabix -p vcf {input[0]}'


rule get_chr_pos:
	'Obtain chr pos for each vcf.'
	input:
		'/mnt/work2/pol/metaGWAS/HLA/data/{cohort}.bgl.phased.vcf.gz'
	output:
		temp('/mnt/work2/pol/metaGWAS/HLA/data/{cohort}.txt')
	shell:
		'zgrep -v "#" {input[0]} | cut -f1-2 > {output[0]}'

rule match_chr_pos:
	''
	input:
		expand('/mnt/work2/pol/metaGWAS/HLA/data/{cohort}.txt', cohort= cohort_nms)
	output:
		'/mnt/work2/pol/metaGWAS/HLA/data/allcohorts.txt'
	run:
		df_list= list()
		from functools import reduce
		for i in range(0, len(input)):
			d= pd.read_csv(input[i], header= None, names= ['CHR', 'POS'], sep= '\t')
			d['x' + str(i)]= 1
			df_list.append(d)
		d= reduce(lambda df1,df2: pd.merge(df1,df2, on= ['CHR', 'POS'], how= 'outer'), df_list)
		d.to_csv(output[0], sep= '\t', header= True, index= False)
			

rule merge_HLA_vcfs:
	'Merge VCF files from different cohorts.'
	input:
		expand('/mnt/work2/pol/metaGWAS/HLA/data/{cohort}.bgl.phased.vcf.gz', cohort= cohort_nms),
		expand('/mnt/work2/pol/metaGWAS/HLA/data/{cohort}.bgl.phased.vcf.gz.tbi', cohort= cohort_nms)
	output:
		'/mnt/work2/pol/metaGWAS/HLA/data/MOBAGENETICS.bgl.phased.vcf.gz'
	run:
		vcf= [i for i in input if i.endswith('vcf.gz')]
		shell('/home/pol.sole.navais/soft/bcftools-1.9/bcftools merge {vcf} | grep -v "./." | gzip > {output[0]}')

rule HLA_assoc:
	'Associations with phenotypes in the merged VCF file using HLA-TAPAS (using PLINK v1.9 under the hood).'
	input:
		'/mnt/work2/pol/metaGWAS/pheno/pheno_all.txt',
                '/mnt/work2/pol/metaGWAS/pheno/covars_all.txt',
		'/mnt/work2/pol/metaGWAS/HLA/data/MOBAGENETICS.bgl.phased.vcf.gz'
	output:
		'/mnt/work2/pol/metaGWAS/HLA/results/{pheno}.txt'
	params:
		'/mnt/work2/pol/metaGWAS/HLA/results/x'
	run:
		if wildcards.pheno in ['GAraw', 'GAnrm']:
			model= 'linear'
		else:
			model= 'logistic.hybrid'
		shell('/home/pol.sole.navais/soft/plink2 --vcf {input[2]} --mac 6 --maf 0.005 --memory 15000 --glm hide-covar cols=+a1freqcc --1 --pheno iid-only {input[0]} --pheno-name {wildcards.pheno} --covar iid-only {input[1]} --covar-col-nums 2-12 --out {params[0]}')
		outPLINK= params[0] + '.' + wildcards.pheno + '.glm.' + model
		shell('mv {outPLINK} {output[0]}')

rule format_manhattan:
	'Format PLINK assoc results for HLA-TAPAS.'
	input:
		'/mnt/work2/pol/metaGWAS/HLA/results/{pheno}.txt'
	output:
		temp('/mnt/work2/pol/metaGWAS/HLA/plots/{pheno}.txt')
	run:
		if wildcards.pheno in ['allPTD', 'postTerm']:
			d= pd.read_csv(input[0], header= 0, sep= '\t', usecols= ['#CHROM', 'POS', 'ID', 'A1', 'TEST', 'OR', 'LOG(OR)_SE', 'Z_STAT', 'P'])
			d.columns= ['CHR', 'BP', 'SNP', 'A1', 'TEST', 'OR', 'SE', 'STAT', 'P']
			d['L95']= d.OR - 1.96 * d['SE']
			d['U95']= d.OR + 1.96 * d['SE']
		else:
			d= pd.read_csv(input[0], header= 0, sep= '\t', usecols= ['#CHROM', 'POS', 'ID', 'A1', 'TEST', 'BETA', 'SE', 'T_STAT', 'P'])
			d.columns= ['CHR', 'BP', 'SNP', 'A1', 'TEST', 'BETA', 'SE', 'STAT', 'P']
		        d['L95']= d.BETA - 1.96 * d['SE']
                        d['U95']= d.BETA + 1.96 * d['SE']
		d.to_csv(output[0], sep= '\t', header= True, index= False)


rule HLA_manhattan_plot:
	'Plot HLA region.'
	input:
		'/mnt/work2/pol/metaGWAS/HLA/plots/{pheno}.txt'
	output:
		'/mnt/work2/pol/metaGWAS/HLA/plots/manhattan_{pheno}.pdf'
	params:
		'/mnt/work2/pol/metaGWAS/HLA/plots/manhattan_{pheno}'
	run:
		if wildcards.pheno in ['allPTD', 'postTerm']:
			infile= input[0] + '.assoc.logistic'
		else:
			infile= input[0] + '.assoc.linear'
		shell('cp {input[0]} {infile}')
		shell('cd /home/pol.sole.navais/soft/HLA-TAPAS/; python -m HLAManhattan --assoc-result {infile} --hg 19 --out {params[0]}')
		shell('rm {infile}')
