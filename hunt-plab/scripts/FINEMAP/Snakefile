import pandas as pd
import numpy as np
import csv
import os


CHR_nms= ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X']

def selectUnrelated(input_kin, df, x):
	kin= pd.read_csv(input_kin, header= 0, delim_whitespace= True, usecols= ['IID1', 'IID2', 'KINSHIP'])
	kin.columns= ['ID1', 'ID2', 'Kinship']
	kin= kin.loc[kin.Kinship > 0.0884, :]
	kin= kin.loc[kin.ID1.isin(x)]
	kin= kin.loc[kin.ID2.isin(x)]
	kin= kin.loc[:, ['ID1','ID2','Kinship']]
	kin_temp= kin.copy()
	kin_temp.columns= ['ID2', 'ID1', 'Kinship']
	kin_temp= kin_temp.append(kin)
	kin_temp['n']= kin_temp.groupby('ID1')['ID1'].transform('count')
	kin_temp['nn']= kin_temp.groupby('ID2')['ID2'].transform('count')
	kin_temp.sort_values(by=['n', 'nn'], inplace= True)
	to_keep= list()
	for i in range(0, len(kin_temp.index)):
		if kin_temp.iloc[i, 0] in kin_temp.iloc[0:i, 1].values:
			kin_temp.iloc[i, 1]= "X"
		else:
			to_keep.append(kin_temp.iloc[i, 0])
	to_remove= [i for i in kin_temp.ID1 if i not in to_keep]
	to_remove= list(set(to_remove))
	remove= pd.DataFrame({'FID': to_remove})
	remove['IID']= remove.FID
	return remove

def maximal_independent_set(input_kin, kin_filter, samples):
	''
	import networkx as nx
	import networkx.algorithms.approximation as nxaa
	df= pd.read_csv(input_kin, header= 0, delim_whitespace= True, usecols= ['IID1', 'IID2', 'KINSHIP'])
	df= df.loc[df.KINSHIP> kin_filter, :]
	df= df.loc[((df.IID1.isin(samples)) & (df.IID2.isin(samples))), :]
	if df.shape[0]== 0: return
	tocheck= set(pd.concat([df.IID1, df.IID2]))
	df= df[['IID1', 'IID2']]
	G = nx.Graph()
	G = nx.from_pandas_edgelist(df, 'IID1', 'IID2')
	z= [0]
	for i in range(50):
		maximal_iset= nx.maximal_independent_set(G)
		if len(maximal_iset) > len(z):
			z= maximal_iset
		else:
			continue
	z= [str(i) for i in z]
	tocheck= [str(i) for i in tocheck]
	toremove= [id for id in tocheck if id not in z]
	return toremove

rule relatedness:
	''
	input:
		'/mnt/archive/hunt/genotypes/plink/genotyped_PID106764.bed'
	output:
		'/mnt/work/hunt/relatedness/all_related.kin0'
	params:
		'/mnt/archive/hunt/genotypes/plink/genotyped_PID106764',
		'/mnt/work/hunt/relatedness/all_related'
	shell:
		'~/soft/plink2 --bfile {params[0]} --maf 0.01 --make-king-table --king-table-filter 0.03125 --not-chr 23,25 --out {params[1]}'


rule sample_ids:
	''
	input:
		'/mnt/archive/hunt/phenotypes/mfr/MFR.txt'
	output:
		'/mnt/work/pol/metaGWAS/processed_data/temp/temp_autosomes_IDS.txt',
		'/mnt/work/pol/metaGWAS/processed_data/temp/temp_chrX_IDS.txt'
	run:
		d= pd.read_csv(input[0], sep= '\t', header= 0)
		x= pd.concat(list([d['FAR_PID'], d['MOR_PID'], d['BARN_PID']]))
		x= x[~x.str.contains('^\s+$')]
		x.to_csv(output[0], header= False, index= False, sep= '\t')
		x= pd.concat(list([d.loc[d.KJONN== 2, 'BARN_PID'], d['MOR_PID']]))
		x= x[~x.str.contains('^\s+$')]
		x.to_csv(output[1], header= False, index= False, sep= '\t')

rule exclude_related:
	''
	input:
		'/mnt/work/pol/metaGWAS/processed_data/temp/temp_autosomes_IDS.txt',
		'/mnt/work/pol/metaGWAS/processed_data/temp/temp_chrX_IDS.txt',
		'/mnt/work/hunt/relatedness/all_related.kin0',
		'/mnt/archive/hunt/genotypes/plink/genotyped_PID106764.fam'
	output:
		'/mnt/work/pol/metaGWAS/processed_data/ids/autosomes_IDS.txt',
		'/mnt/work/pol/metaGWAS/processed_data/ids/chrX_IDS.txt'
	run:
		fam= pd.read_csv(input[3], delim_whitespace= True, header= None, names= ['IID', 'x1', 'x2', 'x3', 'x4', 'x5'])
		fam['IID']= fam.IID.apply(str)
		for i in range(len(output)):
			with open(input[i]) as f:
				d= list(set([line.strip() for line in f]))
				d= [x for x in d if x in fam.IID.values]
			toremove= maximal_independent_set(input[2], 0.0884, d)
			d= [x for x in d if x not in toremove]
			d= pd.DataFrame({'IID': d})
			d.to_csv(output[i], header= False, index= False, sep= '\t')

rule independent_GWAS_regions:
	'Obtain a file with independent regions for top loci with a radius of 1.5 Mb.'
	input:
		'/mnt/work/pol/metaGWAS/results/meta/Maternal_GWAMA_{pheno}.txt.gz',
		'/mnt/work/pol/metaGWAS/processed_data/ids/chrX_IDS.txt'
	output:
		'/mnt/work/pol/metaGWAS/results/top/regions_FINEMAP_{pheno}.txt'
	run:
		d= pd.read_csv(input[0], sep= '\t', compression= 'gzip', usecols= ['CHR', 'POS', 'pvalue', 'nearestGene'])
		df= d.loc[d.pvalue< 5*10**-8, :]
		df.sort_values(by= 'pvalue', ascending= True, inplace= True)
		df.drop_duplicates(subset= ['CHR', 'POS'], keep= 'first', inplace= True)
		df_list= list()
		for chrom in set(df.CHR):
			d_temp= df.loc[df.CHR== chrom, :]
			positions= d_temp.POS.values
			for pos in positions:
				if pos in d_temp.POS.values:
					df_list.append(d_temp.loc[d_temp.POS== pos, :])
					d_temp= d_temp.loc[(d_temp.POS < pos - (1.5*10**6)) | (d_temp.POS> pos + (1.5 * 10**6)), :]
				else:
					continue
		x= pd.concat(df_list)
		x['pos1']= x.POS - 1.5*10**6
		x['pos2']= x.POS + 1.5*10**6
		x['CHR']= x.CHR.astype(str)
		x['CHR']= np.where(x.CHR== '23', 'X', x.CHR)
		x.to_csv(output[0], sep='\t', header= True, index= False, columns= ['CHR', 'pos1', 'pos2', 'nearestGene'])

checkpoint list_variants_sumstats:
	'List of genetic variants (chr:pos) in sumstats file.'
	input:
		'/mnt/work/pol/metaGWAS/results/meta/Maternal_GWAMA_{pheno}.txt.gz',
		'/mnt/work/pol/metaGWAS/results/top/regions_FINEMAP_{pheno}.txt'
	output:
		directory('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/regions')
	params:
		'/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/regions/'
	run:
		if not os.path.exists(params[0]):
			os.makedirs(params[0])
		d= pd.read_csv(input[0], sep= '\t', compression= 'gzip', usecols= ['CHR', 'POS', 'pvalue'])
		d['CHR']= d.CHR.astype(str)
		d['CHR']= np.where(d.CHR== '23', 'X', d.CHR)
		x= pd.read_csv(input[1], sep='\t', header= 0)
		x['CHR']= x.CHR.astype(str)
		d= pd.merge(d, x, on= 'CHR', how= 'inner')
		d= d.loc[((d.POS>= d.pos1) & (d.POS<= d.pos2)), :]
		d.sort_values(by= 'pvalue', ascending= True, inplace= True)
		d.drop_duplicates(subset= ['CHR', 'POS'], keep= 'first', inplace= True)
		d['ID']= d.CHR.map(str) + ':' + d.POS.map(str)
		for gene in set(d.nearestGene):
			temp_df= d.loc[d.nearestGene== gene, :]
			outfile= params[0] + 'chr' + ''.join([str(i) for i in (set(temp_df.CHR))]) + '_' + gene + '.txt'
			with open(outfile, 'w') as f:
				writer= csv.writer(f, delimiter= ' ')
				writer.writerow(temp_df['ID'])

rule filter_vcf:
	'Extract genotypes into BGEN file format for each top locus (3Mb around top SNP).'
	input:
		'/mnt/work/pol/metaGWAS/processed_data/ids/autosomes_IDS.txt',
		'/mnt/work/pol/metaGWAS/processed_data/ids/chrX_IDS.txt',
		'/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/regions/{locus_ID}.txt',
		expand('/mnt/archive/hunt/genotypes/vcf/CHR{CHR}_PID106764.vcf.gz', CHR= CHR_nms)
	output:
		temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/{locus_ID}.vcf.gz'),
		temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/ds/{locus_ID}.vcf.gz'),
		temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/{locus_ID}.vcf.gz.tbi')
	run:
		with open(input[2], 'r') as f:
			chrom= f.readlines()[0].split()[0].split(':')[0]
		if chrom== 'X': chrom= '23'
		if int(chrom) < 10: chrom= '0' + str(chrom)
		print(chrom)
		if chrom== '23': chrom= 'X'
		if chrom== 'X': samples= input[1]
		if chrom!= 'X': samples= input[0]
		vcf= ''.join([infile for infile in input if '/CHR' + chrom + '_PID106764.vcf.gz' in infile])
		shell('/home/pol.sole.navais/soft/qctool/qctool -g {vcf} -incl-samples {samples} -incl-positions {input[2]} -og - | bgzip > {output[0]}')
		shell('tabix -p vcf {output[0]}')
		shell('/home/pol.sole.navais/soft/bcftools/bin/bcftools annotate -x FORMAT/GT {output[0]} -Oz -o {output[1]}')

rule vcf_to_bgen:
	''
	input:
		'/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/ds/{locus_ID}.vcf.gz'
	output:
		temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/bgen/{locus_ID}.bgen'),
		temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/bgen/{locus_ID}.bgen.bgi')
	params:
		'/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/bgen/{locus_ID}'
	shell:
		'''
		/home/pol.sole.navais/soft/plink2 --vcf {input[0]} --export bgen-1.3 --out {params[0]}
		/home/pol.sole.navais/soft/bgen/build/apps/bgenix -g {output[0]} -index 
		'''

rule list_variants_bgen:
        ''
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/bgen/{locus_ID}.bgen',
		'/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/bgen/{locus_ID}.bgen.bgi'
        output:
                temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/variants/{locus_ID}.list')
        shell:
                '/home/pol.sole.navais/soft/bgen/build/apps/bgenix -g {input[0]} -list | sed -n "/alternate_ids/,/# bgenix: success/p" | head -n -1 > {output[0]}'


rule format_sumstats:
        ''
        input:
                '/mnt/work/pol/metaGWAS/results/meta/Maternal_GWAMA_{pheno}.txt.gz',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/variants/{locus_ID}.list'
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/rsids/{locus_ID}_keep_rsid.txt'
        run:
                d= pd.read_csv(input[0], sep= '\t', header=0 , compression= 'gzip', usecols= ['CHR', 'POS', 'EFF', 'REF', 'pvalue'])
                d['CHR']= d['CHR'].apply(str)
                d['CHR']= np.where(d.CHR== '23', 'X', d.CHR)
                x= pd.read_csv(input[1], sep= '\t', header=0, comment= '#', usecols= ['rsid', 'chromosome', 'position', 'first_allele', 'alternative_alleles'])
                x['chromosome']= x['chromosome'].apply(str)
                x= pd.merge(d, x, left_on= ['CHR', 'POS'], right_on= ['chromosome', 'position'], how= 'right')
                x= x.loc[(((x.EFF== x.first_allele) & (x.REF== x.alternative_alleles)) | ((x.EFF== x.alternative_alleles) & (x.REF== x.first_allele))), :]
                x.sort_values(by= 'pvalue', ascending= True, inplace= True)
                x.drop_duplicates(['chromosome', 'position', 'first_allele', 'alternative_alleles'], keep= 'first', inplace= True)
                x.dropna(axis= 0, subset= ['pvalue'], inplace= True)
                with open(output[0], 'w') as f:
                        writer= csv.writer(f, delimiter= ' ')
                        writer.writerow(x['rsid'])


rule extract_bgen_BGENIX:
        'Extract genotypes into BGEN file format for each top locus (3Mb around top SNP).'
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/rsids/{locus_ID}_keep_rsid.txt',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/bgen/{locus_ID}.bgen',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/temp/bgen/{locus_ID}.bgen.bgi'
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/bgen/{locus_ID}.bgen',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/bgen/{locus_ID}.bgen.bgi'
        shell:
                '''
                /home/pol.sole.navais/soft/qctool/qctool -g {input[1]} -incl-rsids {input[0]} -og {output[0]}
                /home/pol.sole.navais/soft/bgen/build/apps/bgenix -g {output[0]} -index
                '''

rule initial_list_variants_bgen:
        'List variants in the same order as index.'
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/bgen/{locus_ID}.bgen',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/bgen/{locus_ID}.bgen.bgi'
        output:
                temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/variants/list_{locus_ID}.txt')
        shell:
                '/home/pol.sole.navais/soft/bgen/build/apps/bgenix -g {input[0]} -list | grep -v "#" > {output}'

rule z_file_LDSTORE:
        ''
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/variants/list_{locus_ID}.txt'
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/list_{locus_ID}.z'
        run:
                d= pd.read_csv(input[0], sep= '\t', header=0, usecols= ['rsid', 'chromosome', 'position', 'first_allele', 'alternative_alleles'])
                d.columns= ['rsid', 'chromosome', 'position', 'allele1', 'allele2']
                d.to_csv(output[0], sep= ' ', header= True, index= False)

rule master_file_LDSTORE:
        'Create a master file for LDSTORE.'
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/list_{locus_ID}.z',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/bgen/{locus_ID}.bgen',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/bgen/{locus_ID}.bgen.bgi',
                '/mnt/work/pol/metaGWAS/processed_data/ids/autosomes_IDS.txt',
                '/mnt/work/pol/metaGWAS/processed_data/ids/chrX_IDS.txt'
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/master/{locus_ID}.master'
        params:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/out/{locus_ID}.bcor',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/out/ld/{locus_ID}.ld',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/out/{locus_ID}.bdose'
        run:
                if 'chrX' in input[0]:
                        n_sample= len([line.strip() for line in open(input[4])])
                        sample= input[4]
                if 'chrX' not in input[0]:
                        n_sample= len([line.strip() for line in open(input[3])])
                        sample= input[3]
                shell("echo 'z;bgen;bgi;bcor;ld;n_samples;bdose' > {output[0]}")
                shell("echo '{input[0]};{input[1]};{input[2]};{params[0]};{params[1]};{n_sample};{params[2]}' >> {output[0]}")

rule bcor_LDSTORE:
        'Compute correlation matrix using LDSTORE.'
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/master/{locus_ID}.master',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/list_{locus_ID}.z',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/bgen/{locus_ID}.bgen',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/bgen/{locus_ID}.bgen.bgi',
                '/mnt/work/pol/metaGWAS/processed_data/ids/autosomes_IDS.txt',
                '/mnt/work/pol/metaGWAS/processed_data/ids/chrX_IDS.txt'
        output:
                temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/out/{locus_ID}.bcor'),
                temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/out/{locus_ID}.bdose')
        threads: 2
        shell:
                '/home/pol.sole.navais/soft/ldstore_v2.0_x86_64/ldstore_v2.0_x86_64 --in-files {input[0]} --write-bcor --write-bdose --bdose-version 1.0 --n-threads {threads} --memory 3'


rule bcor_to_LD_LDSTORE:
        'Write LD file from bcor file using LDSTORE.'
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/master/{locus_ID}.master',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/out/{locus_ID}.bcor'
        output:
                temp('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/out/ld/{locus_ID}.ld')
        shell:
                '/home/pol.sole.navais/soft/ldstore_v2.0_x86_64/ldstore_v2.0_x86_64 --in-files {input[0]} --bcor-to-text'

rule z_file_FINEMAP:
        'Create z file for input in FINEMAP (GWAS sum stats).'
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/list_{locus_ID}.z',
                '/mnt/work/pol/metaGWAS/results/meta/Maternal_GWAMA_{pheno}.txt.gz'
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/zfile/{locus_ID}.z'
        run:
                d= pd.read_csv(input[0], header= 0, sep= ' ')
                d['chromosome']= d.chromosome.map(str)
                x= pd.read_csv(input[1], header= 0, sep= '\t', compression= 'gzip', usecols= ['CHR', 'POS', 'EFF', 'REF', 'EAF', 'BETA', 'SE'])
                x['CHR']= x.CHR.astype(int).astype(str)
                x['CHR']= np.where(x.CHR== '23', 'X', x.CHR)
                x= pd.merge(d, x, left_on= ['chromosome', 'position'], right_on= ['CHR', 'POS'], how= 'left')
                x= x.loc[(((x.EFF== x.allele1) & (x.REF== x.allele2)) | ((x.EFF== x.allele2) & (x.REF== x.allele1))), :]
                x['maf']= np.where(x.EAF> 0.5, 1 - x.EAF, x.EAF)
                x= x[['rsid', 'chromosome', 'position', 'EFF', 'REF', 'maf', 'BETA', 'SE']]
                x.columns= ['rsid', 'chromosome', 'position', 'allele1', 'allele2', 'maf', 'beta', 'se']
                x.drop_duplicates(['chromosome', 'position', 'allele1', 'allele2'], keep= 'first', inplace= True)
                x.dropna(axis= 0, subset= ['beta'], inplace= True)
                x.to_csv(output[0], sep= ' ', header= True, index= False)


rule check_order:
        'Check order of the two z files.'
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/zfile/list_{locus_ID}.z',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/zfile/{locus_ID}.z'
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/order/{locus_ID}.txt'
        run:
                d= pd.read_csv(input[0], header= 0, sep= ' ')
                df= pd.read_csv(input[1], header= 0, sep= ' ')
                if all(d.rsid== df.rsid):
                        open(output[0], 'a').close()
                else:
                        raise ValueError('Check order of z-files')

rule master_file_FINEMAP:
        'Master file for FINEMAP.'
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/zfile/{locus_ID}.z',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/out/ld/{locus_ID}.ld',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/order/{locus_ID}.txt'
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/master/{locus_ID}.master'
        params:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.snp',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.config',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.cred',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.log'
        run:
                if wildcards.pheno== 'allPTD': sample= 172629
                if wildcards.pheno== 'GAraw': sample= 195231
                if wildcards.pheno== 'GAnrm': sample= 149923
                if wildcards.pheno== 'postTerm': sample= 129614
                shell("echo 'z;ld;snp;config;cred;n_samples;log' > {output[0]}")
                shell("echo '{input[0]};{input[1]};{params[0]};{params[1]};{params[2]};{sample};{params[3]}' >> {output[0]}")


rule FINEMAP:
        'Finemapping using FINEMAP'
        input:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/master/{locus_ID}.master',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/LDSTORE/out/ld/{locus_ID}.ld'
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.snp',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.config',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.cred',
                '/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.log_sss'
        shell:
                '''/home/pol.sole.navais/soft/finemap_v1.4_x86_64/finemap_v1.4_x86_64 --in-files {input[0]} --n-causal-snps 5 --sss --log
		mv {output[2]}* {output[2]}
		'''



def aggregate_snp(wildcards):
        'Aggregate the files from locus_ID wildcard.'
        checkpoint_output = checkpoints.list_variants_sumstats.get(**wildcards).output[0]
        return expand('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.snp', pheno= wildcards.pheno, locus_ID= glob_wildcards(os.path.join(checkpoint_output, '{locus_ID}.txt')).locus_ID)

def aggregate_conf(wildcards):
        'Aggregate the files from locus_ID wildcard.'
        checkpoint_output = checkpoints.list_variants_sumstats.get(**wildcards).output[0]
        return expand('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.config', pheno= wildcards.pheno, locus_ID= glob_wildcards(os.path.join(checkpoint_output, '{locus_ID}.txt')).locus_ID)

def aggregate_cred(wildcards):
        'Aggregate the files from locus_ID wildcard.'
        checkpoint_output = checkpoints.list_variants_sumstats.get(**wildcards).output[0]
        return expand('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.cred', pheno= wildcards.pheno, locus_ID= glob_wildcards(os.path.join(checkpoint_output, '{locus_ID}.txt')).locus_ID)

def aggregate_log(wildcards):
        'Aggregate the files from locus_ID wildcard.'
        checkpoint_output = checkpoints.list_variants_sumstats.get(**wildcards).output[0]
        return expand('/mnt/work/pol/metaGWAS/FINEMAP/{pheno}/FINEMAP/results/{locus_ID}.log_sss', pheno= wildcards.pheno, locus_ID= glob_wildcards(os.path.join(checkpoint_output, '{locus_ID}.txt')).locus_ID)


rule merge_snp_outputs:
	''
	input:
		aggregate_snp
	output:
		'/mnt/work/pol/metaGWAS/FINEMAP/results/{pheno}.snp'
	run:
		if len(input)> 1:
			df_list= list()
			for i in input:
				d= pd.read_csv(i, sep= ' ', header=0)
				d['locus']= i.split('results/')[1].replace('.snp', '')
				df_list.append(d)
			d= pd.concat(df_list)
			d.to_csv(output[0], sep= '\t', header= True, index= False)
		else:
			d= pd.read_csv(input[0], sep= ' ', header= 0)
			d['locus']= input[0].split('results/')[1].replace('.snp', '')
			d.to_csv(output[0], sep= '\t', header= True, index= False)

rule merge_conf_outputs:
	''
	input:
		aggregate_conf
	output:
		'/mnt/work/pol/metaGWAS/FINEMAP/results/{pheno}.config'
	run:
		if len(input)> 1:
			df_list= list()
			for i in input:
				d= pd.read_csv(i, sep= ' ', header=0)
				d['locus']= i.split('results/')[1].replace('.config', '')
				df_list.append(d)
			d= pd.concat(df_list)
			d.to_csv(output[0], sep= '\t', header= True, index= False)
		else:
			d= pd.read_csv(input[0], sep= ' ', header=0)
			d['locus']= input[0].split('results/')[1].replace('.config', '')
			d.to_csv(output[0], sep= '\t', header= True, index= False)

rule merge_cred_outputs:
        ''
        input:
                aggregate_cred
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/results/{pheno}.cred'
        shell:
                'tail -n +1 {input} > {output}'

rule merge_log_outputs:
        ''
        input:
                aggregate_log
        output:
                '/mnt/work/pol/metaGWAS/FINEMAP/results/{pheno}.log_sss'
        shell:
                'tail -n +1 {input} > {output}'


rule move_to_github:
	'Move FINEMAP results to github.'
	input:
		'/mnt/work/pol/metaGWAS/FINEMAP/results/{pheno}.{ext}'
	output:
		'FINEMAP/{pheno}.{ext}'
	shell:
		'mv {input[0]} {output[0]}'
